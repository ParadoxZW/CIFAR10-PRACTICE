{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(     #input_size=(1*28*28)\n",
    "            nn.Conv2d(1, 6, 5, 1, 2),\n",
    "            nn.ReLU(),      #(6*28*28)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  #output_size=(6*14*14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),      #(16*10*10)\n",
    "            nn.MaxPool2d(2, 2)  #output_size=(16*5*5)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "dummy_input = torch.rand(13, 1, 28, 28) #假设输入13张1*28*28的图片\n",
    "model = LeNet()\n",
    "with SummaryWriter(comment='LeNet') as w:\n",
    "    w.add_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\"\n",
    "    画出 PyTorch 自动梯度图 autograd graph 的 Graphviz 表示.\n",
    "    蓝色节点表示有梯度计算的变量Variables;\n",
    "    橙色节点表示用于 torch.autograd.Function 中的 backward 的张量 Tensors.\n",
    "\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert all(isinstance(p, Variable) for p in params.values())\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled', shape='box', align='left',\n",
    "                              fontsize='12', ranksep='0.1', height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '(' + (', ').join(['%d' % v for v in size]) + ')'\n",
    "\n",
    "    output_nodes = (var.grad_fn,) if not isinstance(var, tuple) else tuple(v.grad_fn for v in var)\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                # note: this used to show .saved_tensors in pytorch0.2, but stopped\n",
    "                # working as it was moved to ATen and Variable-Tensor merged\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            elif var in output_nodes:\n",
    "                dot.node(str(id(var)), str(type(var).__name__), fillcolor='darkolivegreen1')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "\n",
    "    # 多输出场景 multiple outputs\n",
    "    if isinstance(var, tuple):\n",
    "        for v in var:\n",
    "            add_nodes(v.grad_fn)\n",
    "    else:\n",
    "        add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from ResNet import ResNet\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(     #input_size=(1*28*28)\n",
    "            nn.Conv2d(1, 6, 5, 1, 2),\n",
    "            nn.ReLU(),      #(6*28*28)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  #output_size=(6*14*14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),      #(16*10*10)\n",
    "            nn.MaxPool2d(2, 2)  #output_size=(16*5*5)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    " \n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    " \n",
    "dummy_input = torch.rand(13, 1, 28, 28) #假设输入13张1*28*28的图片\n",
    "model = LeNet()\n",
    "with SummaryWriter(comment='LeNet') as w:\n",
    "    w.add_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-output/test-table.gv.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet(3).double()\n",
    "x = tensor(np.zeros((128, 3, 32, 32))).double()\n",
    "# with SummaryWriter(comment='ResNet') as w:\n",
    "#     w.add_graph(net, (x, ))\n",
    "vise_graph = make_dot(net(x), params=dict(net.named_parameters()))\n",
    "vise_graph.render('test-output/test-table.gv')\n",
    "# y = net(x)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropoutFMP(nn.Module):\n",
    "    \"fractional max pooling with dropout\"\n",
    "    def __init__(self, size, out_channels, dropout=0):\n",
    "        super(DropoutFMP, self).__init__()\n",
    "        self.norm = LayerNorm(features=size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fmp = FMPBlock(size[0], out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.fmp(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FMPNet(nn.Module):\n",
    "    \"implemnet of a cnn network with fractional max pooling\"\n",
    "    def __init__(self):\n",
    "        super(FMPNet, self).__init__()\n",
    "        n = 1\n",
    "        m = 2\n",
    "        self.input = DropoutFMP((3, 32, 32), n)\n",
    "        layers = []\n",
    "        h = 25  # height\n",
    "        k = 1   # times of 160 channels\n",
    "        while h >= 2:\n",
    "            ne = DropoutFMP((n*k, h, h), n*(k+1), 0.045*k)\n",
    "            k += 1\n",
    "            h = int(0.8 * h)\n",
    "            layers.append(ne)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.l1 = nn.Linear(n*11, m)\n",
    "        self.l2 = nn.Linear(m, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.layers(x)\n",
    "        b = x.size()[0]\n",
    "        return self.l2(self.l1(x.view(b, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = FMPNet().double().eval()\n",
    "# x = tensor(np.zeros((1, 3, 32, 32))).double()\n",
    "# y = net(x)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMPNet(\n",
      "  (input): DropoutFMP(\n",
      "    (norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0)\n",
      "    (fmp): FMPBlock(\n",
      "      (c1): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fmp): FractionalMaxPool2d()\n",
      "    )\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.045)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (1): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.09)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(2, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (2): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.135)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (3): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.18)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(4, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (4): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.22499999999999998)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (5): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.27)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(6, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (6): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.315)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (7): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.36)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(8, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (8): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.40499999999999997)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "    (9): DropoutFMP(\n",
      "      (norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.44999999999999996)\n",
      "      (fmp): FMPBlock(\n",
      "        (c1): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (fmp): FractionalMaxPool2d()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (l1): Linear(in_features=11, out_features=2, bias=True)\n",
      "  (l2): Linear(in_features=2, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tensor(np.zeros((1, 3, 32, 32))).double()\n",
    "y = net(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "25\n",
      "20\n",
      "16\n",
      "12\n",
      "9\n",
      "7\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 32  # height\n",
    "channel = 0\n",
    "while h >=1:\n",
    "#     ne = DropoutFMP((channel, h, h), channel + 160)\n",
    "    channel += 1\n",
    "    print(h)\n",
    "    h = math.floor(0.8 * h)\n",
    "channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i += 1\n",
    "y = net(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
